{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "785bf530",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nimare\n",
    "from nimare.extract import download_abstracts, fetch_neuroquery, fetch_neurosynth\n",
    "from nimare.dataset import Dataset\n",
    "from nimare.decode import gclda_decode_roi\n",
    "from nilearn.image import load_img, math_img\n",
    "from nimare.io import convert_neurosynth_to_dataset\n",
    "from nilearn.plotting import plot_roi\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "from nimare.decode import discrete\n",
    "import glob\n",
    "from wordcloud import WordCloud, get_single_color_func\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13d90009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.]\n",
      "(182, 218, 182, 23)\n",
      "Number of networks: 23\n"
     ]
    }
   ],
   "source": [
    "img_vol = nib.load('./data/network_all_035.nii.gz')\n",
    "img_data = img_vol.get_fdata()\n",
    "print(np.unique(img_data))\n",
    "print(img_data.shape)\n",
    "\n",
    "n_networks = img_data.shape[-1]\n",
    "print(f'Number of networks: {n_networks}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0a1ddee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nimare.extract.utils:Dataset found in /data/wheelock/data1/people/Chenyan/NiMare_test/data/supportive_data/neurosynth\n",
      "\n",
      "INFO:nimare.extract.extract:Searching for any feature files matching the following criteria: [('source-abstract', 'vocab-terms', 'data-neurosynth', 'version-7')]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data-neurosynth_version-7_coordinates.tsv.gz\n",
      "File exists and overwrite is False. Skipping.\n",
      "Downloading data-neurosynth_version-7_metadata.tsv.gz\n",
      "File exists and overwrite is False. Skipping.\n",
      "Downloading data-neurosynth_version-7_vocab-terms_source-abstract_type-tfidf_features.npz\n",
      "File exists and overwrite is False. Skipping.\n",
      "Downloading data-neurosynth_version-7_vocab-terms_vocabulary.txt\n",
      "File exists and overwrite is False. Skipping.\n",
      "[{'coordinates': '/data/wheelock/data1/people/Chenyan/NiMare_test/data/supportive_data/neurosynth/data-neurosynth_version-7_coordinates.tsv.gz',\n",
      "  'features': [{'features': '/data/wheelock/data1/people/Chenyan/NiMare_test/data/supportive_data/neurosynth/data-neurosynth_version-7_vocab-terms_source-abstract_type-tfidf_features.npz',\n",
      "                'vocabulary': '/data/wheelock/data1/people/Chenyan/NiMare_test/data/supportive_data/neurosynth/data-neurosynth_version-7_vocab-terms_vocabulary.txt'}],\n",
      "  'metadata': '/data/wheelock/data1/people/Chenyan/NiMare_test/data/supportive_data/neurosynth/data-neurosynth_version-7_metadata.tsv.gz'}]\n"
     ]
    }
   ],
   "source": [
    "out_dir = os.path.abspath(\"./data/supportive_data/\")\n",
    "\n",
    "files = fetch_neurosynth(\n",
    "    data_dir=out_dir,\n",
    "    version=\"7\",\n",
    "    source = \"abstract\",\n",
    "    vocab=\"terms\",\n",
    "    overwrite=False\n",
    ")\n",
    "# Note that the files are saved to a new folder within \"out_dir\" named \"neurosynth\".\n",
    "pprint(files)\n",
    "neurosynth_db = files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39b82797",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:nimare.utils:Not applying transforms to coordinates in unrecognized space 'UNKNOWN'\n"
     ]
    }
   ],
   "source": [
    "neurosynth_dset = convert_neurosynth_to_dataset(\n",
    "    coordinates_file=neurosynth_db[\"coordinates\"],\n",
    "    metadata_file=neurosynth_db[\"metadata\"],\n",
    "    annotations_files=neurosynth_db[\"features\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7d899bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def brain_network_decoding(img, neurosynth_dset, rotation_frame, network_value):\n",
    "    \"\"\"\n",
    "    This function takes an image file, loads it, checks if it's 4D and extracts the appropriate frame,\n",
    "    then applies a transformation based on a scan mask using the Neurosynth dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    img_path (str): Path to the image file (NIfTI format).\n",
    "    neurosynth_dset: The Neurosynth dataset to use for decoding.\n",
    "    rotation_frame (int): Frame index to use if the image is 4D (default is 0).\n",
    "    scan_value (float or int): The value to use for creating the mask from the image (default is 1.5).\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame sorted by 'r' values (association strength).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the image using nibabel\n",
    "    \n",
    "    # Check the shape of the image to determine if it is 4D\n",
    "    img_data = img.get_fdata()\n",
    "    \n",
    "    if img_data.ndim == 4:\n",
    "        # If 4D, extract the frame specified by rotation_frame\n",
    "        img_3d_data = img_data[:, :, :, rotation_frame]\n",
    "    else:\n",
    "        # If not 4D, use the original 3D data\n",
    "        img_3d_data = img_data\n",
    "    \n",
    "    # Create a new Nifti1Image with the extracted data and original affine\n",
    "    img_3d = nib.Nifti1Image(img_3d_data, img.affine)\n",
    "    \n",
    "    # Create a scan mask based on a dynamic condition (e.g., values equal to scan_value)\n",
    "    network_mask = math_img(f\"img == {network_value}\", img=img_3d)\n",
    "    \n",
    "    # Initialize the decoder\n",
    "    decoder = discrete.ROIAssociationDecoder(network_mask)\n",
    "    \n",
    "    # Fit the decoder with the Neurosynth dataset\n",
    "    decoder.fit(neurosynth_dset)\n",
    "    \n",
    "    # Transform the dataset (decoder.transform does not take any parameters)\n",
    "    decoder_df = decoder.transform()\n",
    "    \n",
    "    # Sort the resulting DataFrame by 'r' column in descending order and return the result\n",
    "    return decoder_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0540cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder_network_df = brain_network_decoding(\n",
    "#     img=img_vol,\n",
    "#     neurosynth_dset=neurosynth_dset,\n",
    "#     rotation_frame=0,  # Assuming we want the first frame if it's 4D\n",
    "#     network_value=1.0  # The value to use for the mask\n",
    "# )\n",
    "\n",
    "# decoder_network_df.to_csv('./results/network_decoded.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "128c5e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_networks):\n",
    "    # img_network = np.squeeze(img_vol[:, :, :, i])\n",
    "    decoded_network_df = brain_network_decoding(\n",
    "        img=img_vol,\n",
    "        neurosynth_dset=neurosynth_dset,\n",
    "        rotation_frame=i,  # extract the i-th network\n",
    "        network_value=1.0  # The value to use for the mask\n",
    "    )\n",
    "    decoded_network_df.to_csv(f'./results/23networks_035/network{i+1}_decoded.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_top_k_wordcloud(weights, k=10):\n",
    "#     \"\"\"\n",
    "#     Plot a word cloud of the top-k terms based on their weights.\n",
    "\n",
    "#     Parameters:\n",
    "#     - weights (dict): Dictionary where keys are terms and values are r values (float).\n",
    "#     - k (int): Number of top terms to include.\n",
    "#     - title (str): Title for the plot.\n",
    "#     \"\"\"\n",
    "#     if not weights or k <= 0:\n",
    "#         print(\"Empty input or invalid value of k.\")\n",
    "#         return\n",
    "\n",
    "#     # Sort weights and select top-k\n",
    "#     top_weights = dict(sorted(weights.items(), key=lambda item: item[1], reverse=True)[:k])\n",
    "\n",
    "#     # Generate word cloud\n",
    "#     wordcloud = WordCloud(background_color='white', width=800, height=400).generate_from_frequencies(top_weights)\n",
    "\n",
    "#     # Plot\n",
    "#     plt.figure(figsize=(10, 5))\n",
    "#     plt.imshow(wordcloud, interpolation='bilinear')\n",
    "#     plt.axis('off')\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "def plot_top_k_wordcloud(weights, network_idx, p_threshold, k=10, prefix_to_strip=\"terms_abstract_tfidf__\"):\n",
    "    \"\"\"\n",
    "    Plot a word cloud of the top-k terms based on their weights, stripping a common prefix.\n",
    "\n",
    "    Parameters:\n",
    "    - weights (dict): Dictionary where keys are terms and values are r values (float).\n",
    "    - k (int): Number of top terms to include.\n",
    "    - title (str): Title for the plot.\n",
    "    - prefix_to_strip (str): Prefix to remove from each term.\n",
    "    \"\"\"\n",
    "    if not weights or k <= 0:\n",
    "        print(\"Empty input or invalid value of k.\")\n",
    "        return\n",
    "\n",
    "    # # Remove prefix and sort by r\n",
    "    # cleaned_weights = {\n",
    "    #     key.replace(prefix_to_strip, ''): val\n",
    "    #     for key, val in weights.items()\n",
    "    #     if key.startswith(prefix_to_strip)\n",
    "    # }\n",
    "\n",
    "    # Select top-k\n",
    "    # top_weights = dict(sorted(cleaned_weights.items(), key=lambda item: item[1], reverse=True)[:k])\n",
    "    top_weights = dict(sorted(weights.items(), key=lambda item: item[1], reverse=True)[:k])\n",
    "\n",
    "    color_func = get_single_color_func('darkblue')\n",
    "\n",
    "    # Generate word cloud\n",
    "    wordcloud = WordCloud(\n",
    "        background_color='white', \n",
    "        width=800, height=400, \n",
    "        color_func=color_func\n",
    "        ).generate_from_frequencies(top_weights)\n",
    "    \n",
    "    # Generate and create output directory if needed\n",
    "    output_dir = f'./results/figures_{int(p_threshold * 100)}_k{k}'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.savefig(f'./results/figures_{int(p_threshold*100)}_k{k}/wordcloud_network{network_idx}.png', format='png', dpi=300, bbox_inches='tight')\n",
    "    # plt.show()\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d4e7161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# network1_df = pd.read_csv('./results/23networks_035/network1_decoded.csv')\n",
    "# weights1 = dict(zip(network1_df['feature'], network1_df['r']))\n",
    "\n",
    "# plot_top_k_wordcloud(weights1, k=25, prefix_to_strip=\"terms_abstract_tfidf__\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network2_df = pd.read_csv('./results/23networks_035/network2_decoded.csv')\n",
    "# weights2 = dict(zip(network2_df['feature'], network2_df['r']))\n",
    "\n",
    "# plot_top_k_wordcloud(weights2, k=25, prefix_to_strip=\"terms_abstract_tfidf__\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redundant_suffixes = {\n",
    "    'network', 'networks', 'cortex', 'area', 'areas', 'region', 'regions',\n",
    "    'system', 'systems', 'lobe', 'lobes', 'activation', 'activations',\n",
    "    'brain', 'functional', 'related', 'associated', 'associated_with',\n",
    "    'healthy', 'task', 'tasks', 'connectivity', 'mode'\n",
    "}\n",
    "\n",
    "stop_terms = {\n",
    "    'network', 'networks', 'cortex', 'region', 'system', 'brain',\n",
    "    'activation', 'task', 'mode', 'connectivity'\n",
    "}\n",
    "\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_merge_terms(weights, prefix_to_strip, redundant_suffixes, stop_terms):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    cleaned = defaultdict(float)\n",
    "\n",
    "    for term, val in weights.items():\n",
    "        # Strip prefix and normalize spaces\n",
    "        if not term.startswith(prefix_to_strip):\n",
    "            continue\n",
    "        cleaned_term = term.replace(prefix_to_strip, '')\n",
    "\n",
    "        # Split into words\n",
    "        words = cleaned_term.split(' ')\n",
    "        # Lemmatize each word (singularize, normalize tense)\n",
    "        lemmatized = [lemmatizer.lemmatize(w.lower()) for w in words]\n",
    "\n",
    "        # Remove redundant suffixes like 'network', 'networks', 'cortex'\n",
    "        lemmatized = [w for w in lemmatized if w not in redundant_suffixes]\n",
    "\n",
    "        if not lemmatized:\n",
    "            continue\n",
    "\n",
    "        merged_term = ' '.join(lemmatized)\n",
    "\n",
    "        # Skip generic one-word terms like 'network'\n",
    "        if merged_term in stop_terms:\n",
    "            continue\n",
    "\n",
    "        cleaned[merged_term] = max(val, cleaned[merged_term])\n",
    "\n",
    "    return dict(cleaned)\n",
    "\n",
    "\n",
    "# def suppress_subcomponents(merged_weights):\n",
    "#     final_weights = {}\n",
    "#     multiword_phrases = {k for k in merged_weights if ' ' in k}\n",
    "\n",
    "#     all_suppressed = set()\n",
    "#     for phrase in multiword_phrases:\n",
    "#         components = phrase.split(' ')\n",
    "#         all_suppressed.update(components)\n",
    "\n",
    "#     for term, val in merged_weights.items():\n",
    "#         # If it's a single word that also appears in a multiword phrase, drop it\n",
    "#         if term in all_suppressed and term not in multiword_phrases:\n",
    "#             continue\n",
    "#         final_weights[term] = val\n",
    "\n",
    "#     return final_weights\n",
    "\n",
    "def suppress_lower_components(term_scores):\n",
    "    \"\"\"\n",
    "    Suppress single-word terms that are part of a multi-word phrase\n",
    "    if the multi-word phrase has a higher score.\n",
    "\n",
    "    Parameters:\n",
    "    - term_scores (dict): Dictionary of terms and their scores.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Filtered term_scores with redundant components removed.\n",
    "    \"\"\"\n",
    "    phrases = {term for term in term_scores if ' ' in term}\n",
    "    suppressed = set()\n",
    "\n",
    "    for phrase in phrases:\n",
    "        phrase_score = term_scores[phrase]\n",
    "        parts = phrase.split(' ')\n",
    "\n",
    "        for part in parts:\n",
    "            # Only suppress if the part exists and has a strictly lower score\n",
    "            if part in term_scores and term_scores[part] < phrase_score:\n",
    "                suppressed.add(part)\n",
    "\n",
    "    # Filter out suppressed terms\n",
    "    filtered_scores = {\n",
    "        term: score\n",
    "        for term, score in term_scores.items()\n",
    "        if term not in suppressed\n",
    "    }\n",
    "\n",
    "    return filtered_scores\n",
    "\n",
    "\n",
    "def suppress_bidirectional_overlaps(term_scores, top_k=None):\n",
    "    \"\"\"\n",
    "    Suppress overlapping terms based on relative scores, keeping top-k protected.\n",
    "\n",
    "    - If a phrase is stronger: suppress its parts (unless protected)\n",
    "    - If parts are stronger: suppress the phrase (unless protected)\n",
    "\n",
    "    Parameters:\n",
    "    - term_scores (dict): term -> score\n",
    "    - top_k (int or None): number of top scoring terms to protect\n",
    "\n",
    "    Returns:\n",
    "    - dict: filtered term scores\n",
    "    \"\"\"\n",
    "    safeguard_terms = set()\n",
    "    if top_k is not None:\n",
    "        safeguard_terms = {\n",
    "            term for term, _ in sorted(term_scores.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "        }\n",
    "\n",
    "    suppress_set = set()\n",
    "    phrases = {t for t in term_scores if ' ' in t}\n",
    "\n",
    "    for phrase in phrases:\n",
    "        phrase_score = term_scores[phrase]\n",
    "        parts = phrase.split(' ')\n",
    "\n",
    "        # If phrase is stronger than all parts → suppress parts (unless protected)\n",
    "        if all(\n",
    "            part in term_scores and\n",
    "            term_scores[part] < phrase_score and\n",
    "            part not in safeguard_terms\n",
    "            for part in parts\n",
    "        ):\n",
    "            suppress_set.update(parts)\n",
    "\n",
    "        # If any parts exist and are each stronger than the phrase → suppress phrase\n",
    "        elif any(\n",
    "            part in term_scores and\n",
    "            term_scores[part] > phrase_score\n",
    "            for part in parts\n",
    "        ):\n",
    "            if phrase not in safeguard_terms:\n",
    "                suppress_set.add(phrase)\n",
    "\n",
    "    return {\n",
    "        term: score\n",
    "        for term, score in term_scores.items()\n",
    "        if term not in suppress_set\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing network 1\n",
      "Processing network 2\n",
      "Processing network 3\n",
      "Processing network 4\n",
      "Processing network 5\n",
      "Processing network 6\n",
      "Processing network 7\n",
      "Processing network 8\n",
      "Processing network 9\n",
      "Processing network 10\n",
      "Processing network 11\n",
      "Processing network 12\n",
      "Processing network 13\n",
      "Processing network 14\n",
      "Processing network 15\n",
      "Processing network 16\n",
      "Processing network 17\n",
      "Processing network 18\n",
      "Processing network 19\n",
      "Processing network 20\n",
      "Processing network 21\n",
      "Processing network 22\n",
      "Processing network 23\n",
      "Processing network 1\n",
      "Processing network 2\n",
      "Processing network 3\n",
      "Processing network 4\n",
      "Processing network 5\n",
      "Processing network 6\n",
      "Processing network 7\n",
      "Processing network 8\n",
      "Processing network 9\n",
      "Processing network 10\n",
      "Processing network 11\n",
      "Processing network 12\n",
      "Processing network 13\n",
      "Processing network 14\n",
      "Processing network 15\n",
      "Processing network 16\n",
      "Processing network 17\n",
      "Processing network 18\n",
      "Processing network 19\n",
      "Processing network 20\n",
      "Processing network 21\n",
      "Processing network 22\n",
      "Processing network 23\n",
      "Processing network 1\n",
      "Processing network 2\n",
      "Processing network 3\n",
      "Processing network 4\n",
      "Processing network 5\n",
      "Processing network 6\n",
      "Processing network 7\n",
      "Processing network 8\n",
      "Processing network 9\n",
      "Processing network 10\n",
      "Processing network 11\n",
      "Processing network 12\n",
      "Processing network 13\n",
      "Processing network 14\n",
      "Processing network 15\n",
      "Processing network 16\n",
      "Processing network 17\n",
      "Processing network 18\n",
      "Processing network 19\n",
      "Processing network 20\n",
      "Processing network 21\n",
      "Processing network 22\n",
      "Processing network 23\n"
     ]
    }
   ],
   "source": [
    "for kvals in [10, 15, 25]:\n",
    "    for i in range(n_networks):\n",
    "\n",
    "        print(f'Processing network {i+1}')\n",
    "        network_p35_df = pd.read_csv(f'./results/23networks_035/network{i+1}_decoded.csv')\n",
    "        network_p50_df = pd.read_csv(f'./results/23networks_05/network{i+1}_decoded.csv')\n",
    "\n",
    "        weights35 = dict(zip(network_p35_df['feature'], network_p35_df['r']))\n",
    "        weights50 = dict(zip(network_p50_df['feature'], network_p50_df['r']))\n",
    "\n",
    "        merged_weights35 = clean_and_merge_terms(weights35, \"terms_abstract_tfidf__\", redundant_suffixes, stop_terms)\n",
    "        merged_weights50 = clean_and_merge_terms(weights50, \"terms_abstract_tfidf__\", redundant_suffixes, stop_terms)\n",
    "\n",
    "        # merged_weights35 = suppress_bidirectional_overlaps(merged_weights35)\n",
    "        # merged_weights50 = suppress_bidirectional_overlaps(merged_weights50)\n",
    "\n",
    "        plot_top_k_wordcloud(merged_weights35, i+1, 0.35, k=kvals)\n",
    "        plot_top_k_wordcloud(merged_weights50, i+1, 0.5, k=kvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_networks):\n",
    "\n",
    "    print(f'Processing network {i+1}')\n",
    "    network_p35_df = pd.read_csv(f'./results/23networks_035/network{i+1}_decoded.csv')\n",
    "    network_p50_df = pd.read_csv(f'./results/23networks_05/network{i+1}_decoded.csv')\n",
    "\n",
    "    weights35 = dict(zip(network_p35_df['feature'], network_p35_df['r']))\n",
    "    weights50 = dict(zip(network_p50_df['feature'], network_p50_df['r']))\n",
    "\n",
    "    merged_weights35 = clean_and_merge_terms(weights35, \"terms_abstract_tfidf__\", redundant_suffixes, stop_terms)\n",
    "    merged_weights50 = clean_and_merge_terms(weights50, \"terms_abstract_tfidf__\", redundant_suffixes, stop_terms)\n",
    "\n",
    "    # cleaned_weights35 = suppress_subcomponents(merged_weights35)\n",
    "    # cleaned_weights50 = suppress_subcomponents(merged_weights50)\n",
    "\n",
    "    plot_top_k_wordcloud(merged_weights35, i+1, 0.35, k=25)\n",
    "    plot_top_k_wordcloud(merged_weights50, i+1, 0.5, k=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing network 1\n",
      "Processing network 2\n",
      "Processing network 3\n",
      "Processing network 4\n",
      "Processing network 5\n",
      "Processing network 6\n",
      "Processing network 7\n",
      "Processing network 8\n",
      "Processing network 9\n",
      "Processing network 10\n",
      "Processing network 11\n",
      "Processing network 12\n",
      "Processing network 13\n",
      "Processing network 14\n",
      "Processing network 15\n",
      "Processing network 16\n",
      "Processing network 17\n",
      "Processing network 18\n",
      "Processing network 19\n",
      "Processing network 20\n",
      "Processing network 21\n",
      "Processing network 22\n",
      "Processing network 23\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_networks):\n",
    "\n",
    "    print(f'Processing network {i+1}')\n",
    "    network_p35_df = pd.read_csv(f'./results/23networks_035/network{i+1}_decoded.csv')\n",
    "    network_p50_df = pd.read_csv(f'./results/23networks_05/network{i+1}_decoded.csv')\n",
    "\n",
    "    weights35 = dict(zip(network_p35_df['feature'], network_p35_df['r']))\n",
    "    weights50 = dict(zip(network_p50_df['feature'], network_p50_df['r']))\n",
    "\n",
    "    plot_top_k_wordcloud(weights35, i+1, 0.35, k=15)\n",
    "    plot_top_k_wordcloud(weights50, i+1, 0.5, k=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
